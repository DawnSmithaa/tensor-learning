{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Tensor Factorization with Alternative Least Square (ALS)\n",
    "\n",
    "---\n",
    "**About this chapter**: In many real-world applications, data are multi-dimensional tensors by nature rather than table matrices. In this chapter, we first provide a preliminary overview of tensor factorization family. Then, we provide an tensor factorization implementation using an iterative Alternative Least Square (ALS), which is a good starting point for understanding tensor factorization. Finally, we adapt two public real-world datasets (i.e., Urban traffic speed datasets in Guangzhou, China and Metro station passenger flow datasets in Hangzhou, China) to third-order tensors and evaluate tensor factorization techniques with missing data imputation task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Tensor Factorization Family\n",
    "\n",
    "**1) Tucker Factorization**\n",
    "\n",
    "The idea of tensor decomposition/factorization is to find a low-rank structure approximating the original data. In mathematics, Tucker factorization decomposes a tensor into a set of matrices and one small core tensor [[**wiki**](https://en.wikipedia.org/wiki/Tucker_decomposition)]. Formally, given a third-order tensor $\\mathcal{Y}\\in\\mathbb{R}^{M\\times N\\times T}$, the Tucker form of a tensor (also known as Tucker decomposition/factorization) with low-rank $\\left(R_1,R_2,R_3\\right)$ is defined as\n",
    "\n",
    "$$\\mathcal{Y}\\approx\\mathcal{G}\\times_1 U\\times_2 V\\times_3 X,$$\n",
    "where $\\mathcal{G}\\in\\mathbb{R}^{R_1\\times R_2\\times R_3}$ is core tensor, and $U\\in\\mathbb{R}^{M\\times R_1},V\\in\\mathbb{R}^{N\\times R_2},X\\in\\mathbb{R}^{T\\times R_3}$ are factor matrices.\n",
    "\n",
    "Element-wise, for any $(i,j,t)$-th entry in tensor $\\mathcal{Y}$, the above formula of Tucker factorization can be rewritten as\n",
    "\n",
    "$$y_{ijt}\\approx\\sum_{r_1=1}^{R_1}\\sum_{r_2=1}^{R_2}\\sum_{r_3=1}^{R_3}g_{r_1r_2r_3}u_{ir_1}v_{jr_2}x_{tr_3},$$\n",
    "where $i\\in\\left\\{1,2,...,M\\right\\}$, $j\\in\\left\\{1,2,...,N\\right\\}$, and $t\\in\\left\\{1,2,...,T\\right\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tucker_combine(core_tensor, mat1, mat2, mat3):\n",
    "    import numpy as np\n",
    "    return np.einsum('abc, ia, jb, tc -> ijt', core_tensor, mat1, mat2, mat3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.62385494 0.61868749 0.51256948]\n",
      "  [0.32577527 0.32350121 0.26792985]]\n",
      "\n",
      " [[0.40753966 0.41597023 0.34228418]\n",
      "  [0.21298488 0.21772561 0.17909269]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dim1 = 2\n",
    "dim2 = 2\n",
    "dim3 = 3\n",
    "r1 = 2\n",
    "r2 = 2\n",
    "r3 = 2\n",
    "core_tensor = np.random.rand(r1, r2, r3)\n",
    "mat1 = np.random.rand(dim1, r1)\n",
    "mat2 = np.random.rand(dim2, r2)\n",
    "mat3 = np.random.rand(dim3, r3)\n",
    "tensor = tucker_combine(core_tensor, mat1, mat2, mat3)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) CP Factorization**\n",
    "\n",
    "Another common-used type of tensor factorization is CANDECOMP/PARAFAC (CP) factorization. This form assumes that a data tensor is approximated by a sum of outer products of few factor vectors. Specifically, given a third-order tensor $\\mathcal{Y}\\in\\mathbb{R}^{M\\times N\\times T}$, CP factorization is\n",
    "\n",
    "$$\\mathcal{Y}\\approx\\sum_{r=1}^{R}\\boldsymbol{u}_{r}\\circ\\boldsymbol{v}_{r}\\circ\\boldsymbol{x}_{r},$$\n",
    "where vector $\\boldsymbol{u}_{r}\\in\\mathbb{R}^{M}$ is $r$-th column of factor matrix $U\\in\\mathbb{R}^{M\\times R}$, and there are same definitions for vectors $\\boldsymbol{v}_{r}\\in\\mathbb{R}^{N}$ and $\\boldsymbol{x}_{r}\\in\\mathbb{R}^{T}$ in factor matrices $V\\in\\mathbb{R}^{N\\times R}$ and $X\\in\\mathbb{R}^{T\\times R}$, respectively. In fact, the outer product of these vectors is a rank-one tensor, therefore, we could approximate original data by $R$ rank-one tensors.\n",
    "\n",
    "Element-wise, for any $(i,j,t)$-th entry in tensor $\\mathcal{Y}$, we have\n",
    "\n",
    "$$y_{ijt}\\approx\\sum_{r=1}^{R}u_{ir}v_{jr}x_{tr},$$\n",
    "where $i\\in\\left\\{1,2,...,M\\right\\}$, $j\\in\\left\\{1,2,...,N\\right\\}$, and $t\\in\\left\\{1,2,...,T\\right\\}$. The symbol $\\circ$ denotes vector outer product.\n",
    "\n",
    "- **Example of CP combination**:\n",
    "\n",
    "Given matrices $U=\\left[ \\begin{array}{cc} 1 & 2 \\\\ 3 & 4 \\\\ \\end{array} \\right]\\in\\mathbb{R}^{2\\times 2}$, $V=\\left[ \\begin{array}{cc} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\\\ \\end{array} \\right]\\in\\mathbb{R}^{3\\times 2}$ and $X=\\left[ \\begin{array}{cc} 1 & 5 \\\\ 2 & 6 \\\\ 3 & 7 \\\\ 4 & 8 \\\\ \\end{array} \\right]\\in\\mathbb{R}^{4\\times 2}$, then if $\\hat{\\mathcal{Y}}=\\sum_{r=1}^{R}\\boldsymbol{u}_{r}\\circ\\boldsymbol{v}_{r}\\circ\\boldsymbol{x}_{r}$, then, we have\n",
    "\n",
    "$$\\hat{Y}_1=\\hat{\\mathcal{Y}}(:,:,1)=\\left[ \\begin{array}{ccc} 31 & 42 & 65 \\\\ 63 & 86 & 135 \\\\ \\end{array} \\right],$$\n",
    "$$\\hat{Y}_2=\\hat{\\mathcal{Y}}(:,:,2)=\\left[ \\begin{array}{ccc} 38 & 52 & 82 \\\\ 78 & 108 & 174 \\\\ \\end{array} \\right],$$\n",
    "$$\\hat{Y}_3=\\hat{\\mathcal{Y}}(:,:,3)=\\left[ \\begin{array}{ccc} 45 & 62 & 99 \\\\ 93 & 130 & 213 \\\\ \\end{array} \\right],$$\n",
    "$$\\hat{Y}_4=\\hat{\\mathcal{Y}}(:,:,4)=\\left[ \\begin{array}{ccc} 52 & 72 & 116 \\\\ 108 & 152 & 252 \\\\ \\end{array} \\right].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_combine(mat1, mat2, mat3):\n",
    "    import numpy as np\n",
    "    return np.einsum('ir, jr, tr -> ijt', mat1, mat2, mat3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 31  38  45  52]\n",
      "  [ 42  52  62  72]\n",
      "  [ 65  82  99 116]]\n",
      "\n",
      " [[ 63  78  93 108]\n",
      "  [ 86 108 130 152]\n",
      "  [135 174 213 252]]]\n",
      "\n",
      "tensor size:\n",
      "(2, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "U = np.array([[1, 2], [3, 4]])\n",
    "V = np.array([[1, 3], [2, 4], [5, 6]])\n",
    "X = np.array([[1, 5], [2, 6], [3, 7], [4, 8]])\n",
    "print(cp_combine(U, V, X))\n",
    "print()\n",
    "print('tensor size:')\n",
    "print(cp_combine(U, V, X).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Optimization Problem\n",
    "\n",
    "**1) Tucker Factorization**\n",
    "\n",
    "In Tucker factorization, learning is performed by minimizing the loss function (i.e., sum of residual errors) over core tensor and factor matrices:\n",
    "\n",
    "$$\\min_{\\mathcal{G},U,V,X}\\sum_{(i,j,t)\\in\\Omega}\\left(y_{ijt}-\\sum_{r_1=1}^{R_1}\\sum_{r_2=1}^{R_2}\\sum_{r_3=1}^{R_3}g_{r_1r_2r_3}u_{ir_1}v_{jr_2}x_{tr_3}\\right)^2,$$\n",
    "where index set $\\Omega$ is applied to indicate observed tensor entries.\n",
    "\n",
    "The main challenge of solving this optimization is the need for learning core tensors and factor matrices simultaneously. One way is to iteratively and alternatively update these parameters under a least square framework. We could, for example, consider the following optimization problem for core tensor $\\mathcal{G}$, that is,\n",
    "\n",
    "$$\\min_{\\mathcal{G}}\\sum_{(i,j,t)\\in\\Omega}\\left(y_{ijt}-\\sum_{r_1=1}^{R_1}\\sum_{r_2=1}^{R_2}\\sum_{r_3=1}^{R_3}g_{r_1r_2r_3}u_{ir_1}v_{jr_2}x_{tr_3}\\right)^2,$$\n",
    "\n",
    "$$\\Rightarrow\\min_{\\mathcal{G}}\\sum_{(i,j,t)\\in\\Omega}\\left(y_{ijt}-\\left(\\boldsymbol{x}_{t}\\odot\\boldsymbol{v}_{j}\\odot\\boldsymbol{u}_{i}\\right)^\\top\\text{vec}\\left(\\mathcal{G}\\right)\\right)^2,$$\n",
    "\n",
    "$$\\Rightarrow\\min_{\\mathcal{G}}\\sum_{(i,j,t)\\in\\Omega}\\left(y_{ijt}-\\left(\\boldsymbol{x}_{t}\\odot\\boldsymbol{v}_{j}\\odot\\boldsymbol{u}_{i}\\right)^\\top\\text{vec}\\left(\\mathcal{G}\\right)\\right)^\\top\\left(y_{ijt}-\\left(\\boldsymbol{x}_{t}\\odot\\boldsymbol{v}_{j}\\odot\\boldsymbol{u}_{i}\\right)^\\top\\text{vec}\\left(\\mathcal{G}\\right)\\right),$$\n",
    "where the symbol $\\text{vec}\\left(\\cdot\\right)$ denotes vectorization on matrix (or tensor).\n",
    "\n",
    "---\n",
    "**Theorem 1**: Suppose $d$-th order tensor $\\mathcal{G}\\in\\mathbb{R}^{n_1\\times n_2\\times\\cdots\\times n_d}$ and matrices $U_{k}\\in\\mathbb{R}^{m_k\\times n_k}$ for $k=1,2,...,d$. If the tensor $\\mathcal{A}\\in\\mathbb{R}^{m_1\\times m_2\\times\\cdots\\times m_d}$ is the multi-linear product\n",
    "\n",
    "$$\\mathcal{A}=\\mathcal{G}\\times_1 U_1\\times_2 U_2\\times_3\\cdots\\times_d U_d,$$\n",
    "then\n",
    "\n",
    "$$\\mathcal{A}_{(k)}=U_{k}\\mathcal{G}_{(k)}\\left(U_d\\otimes\\cdots\\otimes U_{k+1}\\otimes U_{k-1}\\otimes\\cdots\\otimes U_1\\right)^\\top,$$\n",
    "and\n",
    "\n",
    "$$\\text{vec}\\left(\\mathcal{A}\\right)=\\left(U_d\\otimes\\cdots\\otimes U_2\\otimes U_1\\right)\\text{vec}\\left(\\mathcal{G}\\right).$$\n",
    "\n",
    "If $U_1,U_2,...,U_d$ are all non-singluar, then $\\mathcal{G}=\\mathcal{A}\\times_1U_1^{-1}\\times_2U_2^{-1}\\times_3\\cdots\\times_dU_d^{-1}$.\n",
    "\n",
    "**Reference**: Gene H. Golub, Charles F. Van Loan, 2012. Matrix Computations (4th Edition). (page: 728-729)\n",
    "\n",
    "---\n",
    "\n",
    "Obviously, as mentioned above, the solution for updating core tensor $\\mathcal{G}$ is\n",
    "\n",
    "$$\\text{vec}\\left(\\mathcal{G}\\right)\\Leftarrow\\left(\\sum_{(i,j,t)\\in\\Omega}\\left(\\boldsymbol{x}_{t}\\odot\\boldsymbol{v}_{j}\\odot\\boldsymbol{u}_{i}\\right)\\left(\\boldsymbol{x}_{t}\\odot\\boldsymbol{v}_{j}\\odot\\boldsymbol{u}_{i}\\right)^\\top\\right)^{-1}\\left(\\sum_{(i,j,t)\\in\\Omega}y_{ijt}\\left(\\boldsymbol{x}_{t}\\odot\\boldsymbol{v}_{j}\\odot\\boldsymbol{u}_{i}\\right)\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
