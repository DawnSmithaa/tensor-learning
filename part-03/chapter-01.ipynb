{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Tensor Factorization with Alternative Least Square (ALS)\n",
    "\n",
    "---\n",
    "**About this chapter**: In many real-world applications, data are multi-dimensional tensors by nature rather than table matrices. In this chapter, we first provide a preliminary overview of tensor factorization family. Then, we provide an tensor factorization implementation using an iterative Alternative Least Square (ALS), which is a good starting point for understanding tensor factorization. Finally, we adapt two public real-world datasets (i.e., Urban traffic speed datasets in Guangzhou, China and Metro station passenger flow datasets in Hangzhou, China) to third-order tensors and evaluate tensor factorization techniques with missing data imputation task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Tensor Factorization Family\n",
    "\n",
    "**1) Tucker Factorization**\n",
    "\n",
    "The idea of tensor decomposition/factorization is to find a low-rank structure approximating the original data. In mathematics, Tucker factorization decomposes a tensor into a set of matrices and one small core tensor [[**wiki**](https://en.wikipedia.org/wiki/Tucker_decomposition)]. Formally, given a third-order tensor $\\mathcal{Y}\\in\\mathbb{R}^{M\\times N\\times T}$, the Tucker form of a tensor (also known as Tucker decomposition/factorization) with low-rank $\\left(R_1,R_2,R_3\\right)$ is defined as\n",
    "\n",
    "$$\\mathcal{Y}\\approx\\mathcal{G}\\times_1 U\\times_2 V\\times_3 X,$$\n",
    "where $\\mathcal{G}\\in\\mathbb{R}^{R_1\\times R_2\\times R_3}$ is core tensor, and $U\\in\\mathbb{R}^{M\\times R_1},V\\in\\mathbb{R}^{N\\times R_2},X\\in\\mathbb{R}^{T\\times R_3}$ are factor matrices.\n",
    "\n",
    "Element-wise, for any $(i,j,t)$-th entry in tensor $\\mathcal{Y}$, the above formula of Tucker factorization can be rewritten as\n",
    "\n",
    "$$y_{ijt}\\approx\\sum_{r_1=1}^{R_1}\\sum_{r_2=1}^{R_2}\\sum_{r_3=1}^{R_3}g_{r_1r_2r_3}u_{ir_1}v_{jr_2}x_{tr_3},$$\n",
    "where $i=1,2,...,M$, $j=1,2,...,N$, and $t=1,2,...,T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def tucker_combine(core_tensor, mat1, mat2, mat3):\n",
    "    return np.einsum('abc, ia, jb, tc -> ijt', core_tensor, mat1, mat2, mat3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.98514589 0.91417508 1.22893368]\n",
      "  [0.06726817 0.0607081  0.08245379]]\n",
      "\n",
      " [[0.68713408 0.64505208 0.86349845]\n",
      "  [0.04707915 0.04274529 0.05792649]]]\n",
      "\n",
      "tensor size:\n",
      "(2, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dim1 = 2\n",
    "dim2 = 2\n",
    "dim3 = 3\n",
    "r1 = 2\n",
    "r2 = 2\n",
    "r3 = 2\n",
    "core_tensor = np.random.rand(r1, r2, r3)\n",
    "mat1 = np.random.rand(dim1, r1)\n",
    "mat2 = np.random.rand(dim2, r2)\n",
    "mat3 = np.random.rand(dim3, r3)\n",
    "tensor = tucker_combine(core_tensor, mat1, mat2, mat3)\n",
    "print(tensor)\n",
    "print()\n",
    "print('tensor size:')\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) CP Factorization**\n",
    "\n",
    "Another common-used type of tensor factorization is CANDECOMP/PARAFAC (CP) factorization. This form assumes that a data tensor is approximated by a sum of outer products of few factor vectors. Specifically, given a third-order tensor $\\mathcal{Y}\\in\\mathbb{R}^{M\\times N\\times T}$, CP factorization is\n",
    "\n",
    "$$\\mathcal{Y}\\approx\\sum_{r=1}^{R}\\boldsymbol{u}_{r}\\circ\\boldsymbol{v}_{r}\\circ\\boldsymbol{x}_{r},$$\n",
    "where vector $\\boldsymbol{u}_{r}\\in\\mathbb{R}^{M}$ is $r$-th column of factor matrix $U\\in\\mathbb{R}^{M\\times R}$, and there are same definitions for vectors $\\boldsymbol{v}_{r}\\in\\mathbb{R}^{N}$ and $\\boldsymbol{x}_{r}\\in\\mathbb{R}^{T}$ in factor matrices $V\\in\\mathbb{R}^{N\\times R}$ and $X\\in\\mathbb{R}^{T\\times R}$, respectively. In fact, the outer product of these vectors is a rank-one tensor, therefore, we could approximate original data by $R$ rank-one tensors.\n",
    "\n",
    "Element-wise, for any $(i,j,t)$-th entry in tensor $\\mathcal{Y}$, we have\n",
    "\n",
    "$$y_{ijt}\\approx\\sum_{r=1}^{R}u_{ir}v_{jr}x_{tr},$$\n",
    "where $i=1,2,...,M$, $j=1,2,...,N$, and $t=1,2,...,T$. The symbol $\\circ$ denotes vector outer product.\n",
    "\n",
    "- **Example of CP combination**:\n",
    "\n",
    "Given matrices $U=\\left[ \\begin{array}{cc} 1 & 2 \\\\ 3 & 4 \\\\ \\end{array} \\right]\\in\\mathbb{R}^{2\\times 2}$, $V=\\left[ \\begin{array}{cc} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\\\ \\end{array} \\right]\\in\\mathbb{R}^{3\\times 2}$ and $X=\\left[ \\begin{array}{cc} 1 & 5 \\\\ 2 & 6 \\\\ 3 & 7 \\\\ 4 & 8 \\\\ \\end{array} \\right]\\in\\mathbb{R}^{4\\times 2}$, then if $\\hat{\\mathcal{Y}}=\\sum_{r=1}^{R}\\boldsymbol{u}_{r}\\circ\\boldsymbol{v}_{r}\\circ\\boldsymbol{x}_{r}$, then, we have\n",
    "\n",
    "$$\\hat{Y}_1=\\hat{\\mathcal{Y}}(:,:,1)=\\left[ \\begin{array}{ccc} 31 & 42 & 65 \\\\ 63 & 86 & 135 \\\\ \\end{array} \\right],$$\n",
    "$$\\hat{Y}_2=\\hat{\\mathcal{Y}}(:,:,2)=\\left[ \\begin{array}{ccc} 38 & 52 & 82 \\\\ 78 & 108 & 174 \\\\ \\end{array} \\right],$$\n",
    "$$\\hat{Y}_3=\\hat{\\mathcal{Y}}(:,:,3)=\\left[ \\begin{array}{ccc} 45 & 62 & 99 \\\\ 93 & 130 & 213 \\\\ \\end{array} \\right],$$\n",
    "$$\\hat{Y}_4=\\hat{\\mathcal{Y}}(:,:,4)=\\left[ \\begin{array}{ccc} 52 & 72 & 116 \\\\ 108 & 152 & 252 \\\\ \\end{array} \\right].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_combine(mat1, mat2, mat3):\n",
    "    import numpy as np\n",
    "    return np.einsum('ir, jr, tr -> ijt', mat1, mat2, mat3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 31  38  45  52]\n",
      "  [ 42  52  62  72]\n",
      "  [ 65  82  99 116]]\n",
      "\n",
      " [[ 63  78  93 108]\n",
      "  [ 86 108 130 152]\n",
      "  [135 174 213 252]]]\n",
      "\n",
      "tensor size:\n",
      "(2, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "U = np.array([[1, 2], [3, 4]])\n",
    "V = np.array([[1, 3], [2, 4], [5, 6]])\n",
    "X = np.array([[1, 5], [2, 6], [3, 7], [4, 8]])\n",
    "print(cp_combine(U, V, X))\n",
    "print()\n",
    "print('tensor size:')\n",
    "print(cp_combine(U, V, X).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Optimization Problem\n",
    "\n",
    "**1) Tucker Factorization**\n",
    "\n",
    "In Tucker factorization, learning is performed by minimizing the loss function (i.e., sum of residual errors) over core tensor and factor matrices:\n",
    "\n",
    "$$\\min_{\\mathcal{G},U,V,X}\\sum_{(i,j,t)\\in\\Omega}\\left(y_{ijt}-\\sum_{r_1=1}^{R_1}\\sum_{r_2=1}^{R_2}\\sum_{r_3=1}^{R_3}g_{r_1r_2r_3}u_{ir_1}v_{jr_2}x_{tr_3}\\right)^2,$$\n",
    "where the index set $\\Omega$ is used to indicate observed tensor entries.\n",
    "\n",
    "The main challenge of solving this optimization is the need for learning core tensors and factor matrices simultaneously. One way is to iteratively and alternatively update these parameters under a least square framework. We could, for example, consider the following optimization problem for core tensor $\\mathcal{G}\\in\\mathbb{R}^{R_1\\times R_2\\times R_3}$, that is,\n",
    "\n",
    "$$\\min_{\\mathcal{G}}\\sum_{(i,j,t)\\in\\Omega}\\left(y_{ijt}-\\sum_{r_1=1}^{R_1}\\sum_{r_2=1}^{R_2}\\sum_{r_3=1}^{R_3}g_{r_1r_2r_3}u_{ir_1}v_{jr_2}x_{tr_3}\\right)^2,$$\n",
    "\n",
    "$$\\Rightarrow\\min_{\\mathcal{G}}\\sum_{(i,j,t)\\in\\Omega}\\left(y_{ijt}-\\left(\\boldsymbol{x}_{t}\\odot\\boldsymbol{v}_{j}\\odot\\boldsymbol{u}_{i}\\right)^\\top\\text{vec}\\left(\\mathcal{G}\\right)\\right)^2,$$\n",
    "\n",
    "$$\\Rightarrow\\min_{\\mathcal{G}}\\sum_{(i,j,t)\\in\\Omega}\\left(y_{ijt}-\\left(\\boldsymbol{x}_{t}\\odot\\boldsymbol{v}_{j}\\odot\\boldsymbol{u}_{i}\\right)^\\top\\text{vec}\\left(\\mathcal{G}\\right)\\right)^\\top\\left(y_{ijt}-\\left(\\boldsymbol{x}_{t}\\odot\\boldsymbol{v}_{j}\\odot\\boldsymbol{u}_{i}\\right)^\\top\\text{vec}\\left(\\mathcal{G}\\right)\\right),$$\n",
    "where the symbol $\\text{vec}\\left(\\cdot\\right)$ denotes vectorization on matrix (or tensor).\n",
    "\n",
    "---\n",
    "**Theorem 1**: Suppose $d$-th order tensor $\\mathcal{G}\\in\\mathbb{R}^{n_1\\times n_2\\times\\cdots\\times n_d}$ and matrices $U_{k}\\in\\mathbb{R}^{m_k\\times n_k}$ for $k=1,2,...,d$. If the tensor $\\mathcal{A}\\in\\mathbb{R}^{m_1\\times m_2\\times\\cdots\\times m_d}$ is the multi-linear product\n",
    "\n",
    "$$\\mathcal{A}=\\mathcal{G}\\times_1 U_1\\times_2 U_2\\times_3\\cdots\\times_d U_d,$$\n",
    "then\n",
    "\n",
    "$$\\mathcal{A}_{(k)}=U_{k}\\mathcal{G}_{(k)}\\left(U_d\\otimes\\cdots\\otimes U_{k+1}\\otimes U_{k-1}\\otimes\\cdots\\otimes U_1\\right)^\\top,$$\n",
    "and\n",
    "\n",
    "$$\\text{vec}\\left(\\mathcal{A}\\right)=\\left(U_d\\otimes\\cdots\\otimes U_2\\otimes U_1\\right)\\text{vec}\\left(\\mathcal{G}\\right).$$\n",
    "\n",
    "If $U_1,U_2,...,U_d$ are all non-singluar, then $\\mathcal{G}=\\mathcal{A}\\times_1U_1^{-1}\\times_2U_2^{-1}\\times_3\\cdots\\times_dU_d^{-1}$.\n",
    "\n",
    "**Reference**: Gene H. Golub, Charles F. Van Loan, 2012. Matrix Computations (4th Edition). (page: 728-729)\n",
    "\n",
    "---\n",
    "\n",
    "Obviously, the solution for updating core tensor $\\mathcal{G}$ is\n",
    "\n",
    "$$\\text{vec}\\left(\\mathcal{G}\\right)\\Leftarrow\\left(\\sum_{(i,j,t)\\in\\Omega}\\left(\\boldsymbol{x}_{t}\\odot\\boldsymbol{v}_{j}\\odot\\boldsymbol{u}_{i}\\right)\\left(\\boldsymbol{x}_{t}\\odot\\boldsymbol{v}_{j}\\odot\\boldsymbol{u}_{i}\\right)^\\top\\right)^{-1}\\left(\\sum_{(i,j,t)\\in\\Omega}y_{ijt}\\left(\\boldsymbol{x}_{t}\\odot\\boldsymbol{v}_{j}\\odot\\boldsymbol{u}_{i}\\right)\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to core tensor $\\mathcal{G}$, the optimization problem for factor matrix $U\\in\\mathbb{R}^{M\\times R_1}$ can be written as follows,\n",
    "\n",
    "$$\\min_{U}\\sum_{(i,j,t)\\in\\Omega}\\left(y_{ijt}-\\sum_{r_1=1}^{R_1}\\sum_{r_2=1}^{R_2}\\sum_{r_3=1}^{R_3}g_{r_1r_2r_3}u_{ir_1}v_{jr_2}x_{tr_3}\\right)^2,$$\n",
    "\n",
    "$$\\Rightarrow\\min_{\\boldsymbol{u}_i}\\sum_{j,t:(i,j,t)\\in\\Omega}\\left(y_{ijt}-\\boldsymbol{u}_i^\\top\\mathcal{G}_{(1)}\\left(\\boldsymbol{x}_{t}\\odot\\boldsymbol{v}_{j}\\right)\\right)\\left(y_{ijt}-\\boldsymbol{u}_i^\\top\\mathcal{G}_{(1)}\\left(\\boldsymbol{x}_{t}\\odot\\boldsymbol{v}_{j}\\right)\\right)^\\top.$$\n",
    "\n",
    "In such case, we could derive the least square as\n",
    "\n",
    "$$\\boldsymbol{u}_{i}\\Leftarrow\\left(\\sum_{j,t:(i,j,t)\\in\\Omega}\\mathcal{G}_{(1)}\\left(\\boldsymbol{x}_{t}\\odot\\boldsymbol{v}_{j}\\right)\\left(\\boldsymbol{x}_{t}\\odot\\boldsymbol{v}_{j}\\right)^\\top\\mathcal{G}_{(1)}^\\top\\right)^{-1}\\left(\\sum_{j,t:(i,j,t)\\in\\Omega}y_{ijt}\\mathcal{G}_{(1)}\\left(\\boldsymbol{x}_{t}\\odot\\boldsymbol{v}_{j}\\right)\\right),\\forall i\\in\\left\\{1,2,...,M\\right\\}.$$\n",
    "\n",
    "The alternative least squares for $V\\in\\mathbb{R}^{N\\times R_2}$ and $X\\in\\mathbb{R}^{T\\times R_3}$ are\n",
    "\n",
    "$$\\boldsymbol{v}_{j}\\Leftarrow\\left(\\sum_{i,t:(i,j,t)\\in\\Omega}\\mathcal{G}_{(2)}\\left(\\boldsymbol{x}_{t}\\odot\\boldsymbol{u}_{i}\\right)\\left(\\boldsymbol{x}_{t}\\odot\\boldsymbol{u}_{i}\\right)^\\top\\mathcal{G}_{(2)}^\\top\\right)^{-1}\\left(\\sum_{i,t:(i,j,t)\\in\\Omega}y_{ijt}\\mathcal{G}_{(2)}\\left(\\boldsymbol{x}_{t}\\odot\\boldsymbol{u}_{i}\\right)\\right),\\forall j\\in\\left\\{1,2,...,N\\right\\},$$\n",
    "\n",
    "$$\\boldsymbol{x}_{t}\\Leftarrow\\left(\\sum_{i,j:(i,j,t)\\in\\Omega}\\mathcal{G}_{(3)}\\left(\\boldsymbol{v}_{j}\\odot\\boldsymbol{u}_{i}\\right)\\left(\\boldsymbol{v}_{j}\\odot\\boldsymbol{u}_{i}\\right)^\\top\\mathcal{G}_{(3)}^\\top\\right)^{-1}\\left(\\sum_{i,j:(i,j,t)\\in\\Omega}y_{ijt}\\mathcal{G}_{(3)}\\left(\\boldsymbol{v}_{j}\\odot\\boldsymbol{u}_{i}\\right)\\right),\\forall t\\in\\left\\{1,2,...,T\\right\\}.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Prerequisite functions:'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def kr_prod(a, b):\n",
    "    return np.einsum('ir, jr -> ijr', a, b).reshape(a.shape[0] * b.shape[0], -1)\n",
    "\n",
    "def ten2mat(tensor, mode):\n",
    "    return np.reshape(np.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1), order = 'F')\n",
    "\n",
    "def mat2ten(mat, tensor_size, mode):\n",
    "    index = list()\n",
    "    index.append(mode)\n",
    "    for i in range(tensor_size.shape[0]):\n",
    "        if i != mode:\n",
    "            index.append(i)\n",
    "    return np.moveaxis(np.reshape(mat, list(tensor_size[index]), order = 'F'), 0, mode)\n",
    "\n",
    "def mat2vec(mat):\n",
    "    dim1, dim2 = mat.shape\n",
    "    return mat.T.reshape([dim1 * dim2])\n",
    "\n",
    "def vec2mat(vec, mat_size):\n",
    "    return vec.reshape([mat_size[1], mat_size[0]]).T\n",
    "\n",
    "def Tucker_ALS(sparse_tensor, rank, maxiter):\n",
    "    dim1, dim2, dim3 = sparse_tensor.shape\n",
    "    rank1 = rank[0]\n",
    "    rank2 = rank[1]\n",
    "    rank3 = rank[2]\n",
    "    \n",
    "    G = 0.1 * np.random.rand(rank1, rank2, rank3)\n",
    "    U = 0.1 * np.random.rand(dim1, rank1)\n",
    "    V = 0.1 * np.random.rand(dim2, rank2)\n",
    "    X = 0.1 * np.random.rand(dim3, rank3)\n",
    "    \n",
    "    pos = np.where(sparse_tensor != 0)\n",
    "    binary_tensor = np.zeros((dim1, dim2, dim3))\n",
    "    binary_tensor[pos] = 1\n",
    "    tensor_hat = np.zeros((dim1, dim2, dim3))\n",
    "    \n",
    "    for iters in range(maxiter):\n",
    "        small_mat = np.zeros((rank1 * rank2 * rank3, rank1 * rank2 * rank3))\n",
    "        small_vec = np.zeros((rank1 * rank2 * rank3))\n",
    "        for ind in range(pos[0].shape[0]):\n",
    "            vec0 = kr_prod(kr_prod(X[pos[2][ind], :].reshape([rank3, 1]), \n",
    "                                   V[pos[1][ind], :].reshape([rank2, 1])), \n",
    "                           U[pos[0][ind], :].reshape([rank1, 1]))\n",
    "            vec0 = vec0.reshape([rank1 * rank2 * rank3])\n",
    "            small_mat += np.outer(vec0, vec0)\n",
    "            small_vec += sparse_tensor[pos[0][ind], pos[1][ind], pos[2][ind]] * vec0\n",
    "        small_mat = (small_mat + small_mat.T)/2\n",
    "        G_vec = np.matmul(np.linalg.inv(small_mat), small_vec)\n",
    "        G = mat2ten(vec2mat(G_vec, np.array([rank1, rank2 * rank3])), np.array([rank1, rank2, rank3]), 0)\n",
    "\n",
    "        G1 = ten2mat(G, 0)\n",
    "        var1 = np.matmul(G1, np.kron(X, V).T)\n",
    "        var2 = kr_prod(var1, var1)\n",
    "        var3 = np.matmul(var2, ten2mat(binary_tensor, 0).T).reshape([rank1, rank1, dim1])\n",
    "        var4 = np.matmul(var1, ten2mat(sparse_tensor, 0).T)\n",
    "        for i in range(dim1):\n",
    "            var_Lambda = var3[ :, :, i]\n",
    "            inv_var_Lambda = np.linalg.inv((var_Lambda + var_Lambda.T)/2)\n",
    "            U[i, :] = np.matmul(inv_var_Lambda, var4[:, i])\n",
    "\n",
    "        G2 = ten2mat(G, 1)\n",
    "        var1 = np.matmul(G2, np.kron(X, U).T)\n",
    "        var2 = kr_prod(var1, var1)\n",
    "        var3 = np.matmul(var2, ten2mat(binary_tensor, 1).T).reshape([rank2, rank2, dim2])\n",
    "        var4 = np.matmul(var1, ten2mat(sparse_tensor, 1).T)\n",
    "        for j in range(dim2):\n",
    "            var_Lambda = var3[ :, :, j]\n",
    "            inv_var_Lambda = np.linalg.inv((var_Lambda + var_Lambda.T)/2)\n",
    "            V[j, :] = np.matmul(inv_var_Lambda, var4[:, j])\n",
    "\n",
    "        G3 = ten2mat(G, 2)\n",
    "        var1 = np.matmul(G3, np.kron(V, U).T)\n",
    "        var2 = kr_prod(var1, var1)\n",
    "        var3 = np.matmul(var2, ten2mat(binary_tensor, 2).T).reshape([rank3, rank3, dim3])\n",
    "        var4 = np.matmul(var1, ten2mat(sparse_tensor, 2).T)\n",
    "        for t in range(dim3):\n",
    "            var_Lambda = var3[ :, :, t]\n",
    "            inv_var_Lambda = np.linalg.inv((var_Lambda + var_Lambda.T)/2)\n",
    "            X[t, :] = np.matmul(inv_var_Lambda, var4[:, t])\n",
    "\n",
    "        tensor_hat = tucker_combine(G, U, V, X)\n",
    "        mape = np.sum(np.abs(sparse_tensor[pos] - tensor_hat[pos])/sparse_tensor[pos])/sparse_tensor[pos].shape[0]\n",
    "        rmse = np.sqrt(np.sum((sparse_tensor[pos] - tensor_hat[pos]) ** 2)/sparse_tensor[pos].shape[0])\n",
    "        \n",
    "        if (iters + 1) % 5 == 0:\n",
    "            print('Iter: {}'.format(iters + 1))\n",
    "            print('Training MAPE: {:.6}'.format(mape))\n",
    "            print('Training RMSE: {:.6}'.format(rmse))\n",
    "            print()\n",
    "    \n",
    "    return tensor_hat, G, U, V, X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Example: Traffic Volume Data Imputation**\n",
    "\n",
    "Given a small traffic volume dataset with form of third-order tensor $\\mathcal{X}\\in\\mathbb{R}^{7\\times 4\\times 3}$, i.e.,\n",
    "\n",
    "$$\\mathcal{X}(:, :, 1)=\\left[\\begin{array}{cccc}{155} & {74} & {493} & {426} \\\\ {108} & {44} & {350} & {359} \\\\ {175} & {78} & {567} & {581} \\\\ {181} & {111} & {517} & {552} \\\\ {137} & {53} & {489} & {485} \\\\ {90} & {44} & {306} & {290} \\\\ {139} & {55} & {398} & {390}\\end{array}\\right],\\mathcal{X}( :, :, 2)=\\left[\\begin{array}{cccc}{172} & {69} & {590} & {386} \\\\ {104} & {39} & {310} & {304} \\\\ {158} & {74} & {505} & {546} \\\\ {176} & {90} & {525} & {552} \\\\ {150} & {64} & {438} & {459} \\\\ {73} & {32} & {281} & {299} \\\\ {127} & {51} & {358} & {382}\\end{array}\\right],\\mathcal{X}( :, :, 3)=\\left[\\begin{array}{cccc}{225} & {92} & {443} & {436} \\\\ {94} & {44} & {355} & {356} \\\\ {139} & {77} & {575} & {604} \\\\ {175} & {98} & {574} & {553} \\\\ {126} & {67} & {593} & {484} \\\\ {58} & {49} & {348} & {301} \\\\ {144} & {71} & {444} & {396}\\end{array}\\right],$$\n",
    "where 7 indicates 7 spatial locations (or sensors), 4 indicates 4 days, and 3 indicates 3 15-minute time intervals. The unit of tensor entries is vehicle per 15 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim1 = 7\n",
    "dim2 = 4\n",
    "dim3 = 3\n",
    "dense_tensor = np.zeros((dim1, dim2, dim3))\n",
    "dense_tensor[:, :, 0] = np.array([[155, 74, 493, 426], [108, 44, 350, 359],\n",
    "                                  [175, 78, 567, 581], [181, 111, 517, 552],\n",
    "                                  [137, 53, 489, 485], [90, 44, 306, 290],\n",
    "                                  [139, 55, 398, 390]])\n",
    "dense_tensor[:, :, 1] = np.array([[172, 69, 590, 386], [104, 39, 310, 304],\n",
    "                                  [158, 74, 505, 546], [176, 90, 525, 552],\n",
    "                                  [150, 64, 438, 459], [73, 32, 281, 299],\n",
    "                                  [127, 51, 358, 382]])\n",
    "dense_tensor[:, :, 2] = np.array([[225, 92, 443, 436], [94, 44, 355, 356],\n",
    "                                  [139, 77, 575, 604], [175, 98, 574, 553],\n",
    "                                  [126, 67, 593, 484], [58, 49, 348, 301],\n",
    "                                  [144, 71, 444, 396]])\n",
    "sparse_tensor = np.zeros((dim1, dim2, dim3))\n",
    "sparse_tensor[:, :, 0] = np.array([[155, 74, 493, 426], [108, 0, 0, 0],\n",
    "                                   [175, 78, 0, 0], [0, 111, 517, 0],\n",
    "                                   [137, 53, 489, 0], [90, 44, 0, 0],\n",
    "                                   [139, 0, 398, 0]])\n",
    "sparse_tensor[:, :, 1] = np.array([[172, 69, 590, 0], [104, 0, 310, 304],\n",
    "                                   [0, 0, 505, 546], [0, 90, 525, 552],\n",
    "                                   [0, 64, 0, 0], [73, 32, 281, 299],\n",
    "                                   [127, 0, 0, 0]])\n",
    "sparse_tensor[:, :, 2] = np.array([[225, 0, 0, 436], [0, 44, 0, 356],\n",
    "                                   [0, 0, 575, 604], [175, 98, 574, 0],\n",
    "                                   [126, 67, 0, 0], [58, 49, 348, 0],\n",
    "                                   [144, 0, 444, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 5\n",
      "Training MAPE: 0.105222\n",
      "Training RMSE: 25.7651\n",
      "\n",
      "Iter: 10\n",
      "Training MAPE: 0.105465\n",
      "Training RMSE: 25.7619\n",
      "\n",
      "Iter: 15\n",
      "Training MAPE: 0.105465\n",
      "Training RMSE: 25.7619\n",
      "\n",
      "Iter: 20\n",
      "Training MAPE: 0.105465\n",
      "Training RMSE: 25.7619\n",
      "\n",
      "Final Imputation MAPE: 0.109442\n",
      "Final Imputation RMSE: 40.2588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rank1 = 1\n",
    "rank2 = 1\n",
    "rank3 = 1\n",
    "rank = np.array([rank1, rank2, rank3])\n",
    "maxiter = 20\n",
    "tensor_hat, G, U, V, X = Tucker_ALS(sparse_tensor, rank, maxiter)\n",
    "pos = np.where((dense_tensor != 0) & (sparse_tensor == 0))\n",
    "final_mape = np.sum(np.abs(dense_tensor[pos] - tensor_hat[pos])/dense_tensor[pos])/dense_tensor[pos].shape[0]\n",
    "final_rmse = np.sqrt(np.sum((dense_tensor[pos] - tensor_hat[pos]) ** 2)/dense_tensor[pos].shape[0])\n",
    "print('Final Imputation MAPE: {:.6}'.format(final_mape))\n",
    "print('Final Imputation RMSE: {:.6}'.format(final_rmse))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) CP Factorization**\n",
    "\n",
    "Indeed, the formula of CP factorization is a special case of Tucker factorization. Mathematically, for any $(i,j,t)$-th entry of a given third-order tensor $\\mathcal{Y}$, the form of CP factorization can be written as\n",
    "\n",
    "$$y_{ijt}\\approx\\sum_{r=1}^{R}u_{ir}v_{jr}x_{tr}=\\sum_{r_1=1}^{R}\\sum_{r_2=1}^{R}\\sum_{r_3=1}^{R}g_{r_1r_2r_3}u_{ir_1}v_{jr_2}x_{jr_3},$$\n",
    "where hyper-diagonal entries of the core tensor $\\mathcal{G}$ are 1. In other words, $g_{r_1r_2r_3}=1$ for any $r_1=r_2=r_3$ and $g_{r_1r_2r_3}=0$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding CP factorization as a machine learning problem, we could perform a learning task by minimizing the loss function over factor matrices like aforementioned Tucker factorization, that is,\n",
    "\n",
    "$$\\min _{U, V, X} \\sum_{(i, j, t) \\in \\Omega}\\left(y_{i j t}-\\sum_{r=1}^{R}u_{ir}v_{jr}x_{tr}\\right)^{2}.$$\n",
    "\n",
    "Within this optimization problem, multiplication among three factor matrices (acted as parameters) makes this problem difficult. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Non-Negative Tensor Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Spatiotemporal Data Imputation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix A1: Tucker Factorization on the Guangzhou Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "tensor = scipy.io.loadmat('../Guangzhou-data-set/tensor.mat')\n",
    "dense_tensor = tensor['tensor']\n",
    "random_matrix = scipy.io.loadmat('../Guangzhou-data-set/random_matrix.mat')\n",
    "random_matrix = random_matrix['random_matrix']\n",
    "random_tensor = scipy.io.loadmat('../Guangzhou-data-set/random_tensor.mat')\n",
    "random_tensor = random_tensor['random_tensor']\n",
    "\n",
    "missing_rate = 0.4\n",
    "\n",
    "# =============================================================================\n",
    "### Random missing (RM) scenario\n",
    "### Set the RM scenario by:\n",
    "# binary_tensor = np.round(random_tensor + 0.5 - missing_rate)\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "### Non-random missing (NM) scenario\n",
    "### Set the NM scenario by:\n",
    "binary_tensor = np.zeros(dense_tensor.shape)\n",
    "for i1 in range(dense_tensor.shape[0]):\n",
    "    for i2 in range(dense_tensor.shape[1]):\n",
    "        binary_tensor[i1,i2,:] = np.round(random_matrix[i1,i2] + 0.5 - missing_rate)\n",
    "# =============================================================================\n",
    "\n",
    "sparse_tensor = np.multiply(dense_tensor, binary_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 5\n",
      "Training MAPE: 0.105343\n",
      "Training RMSE: 4.36716\n",
      "\n",
      "Iter: 10\n",
      "Training MAPE: 0.104836\n",
      "Training RMSE: 4.35449\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "rank1 = 7\n",
    "rank2 = 7\n",
    "rank3 = 7\n",
    "rank = np.array([rank1, rank2, rank3])\n",
    "maxiter = 20\n",
    "tensor_hat, G, U, V, X = Tucker_ALS(sparse_tensor, rank, maxiter)\n",
    "pos = np.where((dense_tensor != 0) & (sparse_tensor == 0))\n",
    "final_mape = np.sum(np.abs(dense_tensor[pos] - tensor_hat[pos])/dense_tensor[pos])/dense_tensor[pos].shape[0]\n",
    "final_rmse = np.sqrt(np.sum((dense_tensor[pos] - tensor_hat[pos]) ** 2)/dense_tensor[pos].shape[0])\n",
    "print('Final Imputation MAPE: {:.6}'.format(final_mape))\n",
    "print('Final Imputation RMSE: {:.6}'.format(final_rmse))\n",
    "print()\n",
    "\n",
    "end = time.time()\n",
    "print('Running time: %d seconds.'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  scenario |    `rank`| `maxiter`|       mape |      rmse |\n",
    "|:----------|---------:|---------:|-----------:|----------:|\n",
    "|**40%, NM**|   (2,2,2)|       20 |     0.1319 |    5.2762 |\n",
    "|**40%, NM**|   (3,3,3)|       20 |     0.1210 |    4.8816 |\n",
    "|**40%, NM**|   (4,4,4)|       20 |     0.1129 |    4.6304 |\n",
    "|**40%, NM**|   (5,5,5)|       20 |     0.1095 |    4.5260 |\n",
    "|**40%, NM**|   (6,6,6)|       20 |     0.1071 |    4.4605 |\n",
    "|**40%, NM**|   (7,7,7)|       20 |     0.1049 |    4.4038 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
