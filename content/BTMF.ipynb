{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Temporal Matrix Factorization\n",
    "\n",
    "**Published**: October 8, 2019\n",
    "\n",
    "**Author**: Xinyu Chen [[**GitHub homepage**](https://github.com/xinychen)]\n",
    "\n",
    "**Download**: This Jupyter notebook is at our GitHub repository. If you want to evaluate the code, please download the notebook from the repository of [**tensor-learning**](https://github.com/xinychen/tensor-learning/blob/master/content/BTMF.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Large-scale and multidimensional spatiotemporal data sets are becoming ubiquitous in many real-world applications such as monitoring traffic and air quality. Making predictions on these time series has become a critical challenge due to not only the large-scale and high-dimensional nature but also the considerable amount of missing data. In this work, we propose a Bayesian Temporal Matrix Factorization (BTMF) model for modeling multidimensional time series - and in particular spatiotemporal data - in the presence of missing data. By integrating low-rank matrix factorization and vector autoregressive (VAR) process into a single probabilistic graphical model, our model can effectively perform predictions without imputing those missing values. We develop efficient Gibbs sampling algorithms for model inference and test the proposed BTMF on several real-world spatiotemporal data sets for both missing data imputation and short-term rolling prediction tasks. This post is mainly about BTMF models and their **`Python`** implementation with an application of spatiotemporal data imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Motivation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Problem Description\n",
    "\n",
    "We assume a spatiotemporal setting for multidimensional time series data throughout this work. In general, modern spatiotemporal data sets collected from sensor networks can be organized as matrix time series. For example, we can denote by matrix $Y\\in\\mathbb{R}^{N\\times T}$ a multivariate time series collected from $N$ locations/sensors on $T$ time stamps, with each row $$\\boldsymbol{y}_{i}=\\left(y_{i,1},y_{i,2},...,y_{i,t-1},y_{i,t},y_{i,t+1},...,y_{i,T}\\right)$$\n",
    "corresponding to the time series collected at location $i$.\n",
    "\n",
    "As mentioned, making accurate predictions on incomplete time series is very challenging, while missing data problem is almost inevitable in real-world applications. Figure 1 illustrates the prediction problem for incomplete time series data. Here we use $(i,t)\\in\\Omega$ to index the observed entries in matrix $Y$.\n",
    "\n",
    "<img src=\"../images/graphical_matrix_time_series.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "> **Figure 1**: Illustration of multivariate time series and the prediction problem in the presence of missing values (green: observed data; white: missing data; red: prediction).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Model Description\n",
    "\n",
    "Given a partially observed spatiotemporal matrix $Y\\in\\mathbb{R}^{N \\times T}$, one can factorize it into a spatial factor matrix $W\\in\\mathbb{R}^{R \\times N}$ and a temporal factor matrix $X\\in\\mathbb{R}^{R \\times T}$ following general matrix factorization model:\n",
    "$$Y\\approx W^{\\top}X,~~~~(1)$$\n",
    "and element-wise, we have\n",
    "$$y_{it}\\approx \\boldsymbol{w}_{i}^\\top\\boldsymbol{x}_{t}, \\quad \\forall (i,t),~~~~(2)$$\n",
    "where vectors $\\boldsymbol{w}_{i}$ and $\\boldsymbol{x}_{t}$ refer to the $i$-th column of $W$ and the $t$-th column of $X$, respectively.\n",
    "\n",
    "The standard matrix factorization model is a good approach to deal with the missing data problem; however, it cannot capture the dependencies among different columns in $X$, which are critical in modeling time series data. To better characterize the temporal dependencies and impose temporal smoothness, a novel AR regularizer is introduced on $X$ in TRMF (i.e., Temporal Regularizer Matrix Factorization proposed by [Yu et al., 2016](https://www.cs.utexas.edu/~rofuyu/papers/tr-mf-nips.pdf)):\n",
    "$$\\begin{array}{l}\\boldsymbol{x}_{t+1}&=\\sum\\nolimits_{k=1}^{d}A_{k}\\boldsymbol{x}_{t+1-h_k}+\\boldsymbol{\\epsilon}_t, \\\\\n",
    "    &=A^\\top \\boldsymbol{v}_{t+1}+\\boldsymbol{\\epsilon}_{t}, \\\\ \\end{array}~~~~(3)$$\n",
    "where $\\mathcal{L}=\\left\\{h_1,\\ldots,h_k,\\ldots,h_d\\right\\}$ is a lag set ($d$ is the order of this AR model), each $A_k$ ($k\\in\\left\\{1,...,d\\right\\}$) is a $R\\times R$ coefficient matrix, and $\\boldsymbol{\\epsilon}_t$ is a zero mean Gaussian noise vector. For brevity, matrix $A\\in \\mathbb{R}^{(R d) \\times R}$ and vector $\\boldsymbol{v}_{t+1}\\in \\mathbb{R}^{(R d) \\times 1}$ are defined as\n",
    "$$A=\\left[A_{1}, \\ldots, A_{d}\\right]^{\\top} ,\\quad \\boldsymbol{v}_{t+1}=\\left[\\begin{array}{c}{\\boldsymbol{x}_{t+1-h_1}} \\\\ {\\vdots} \\\\ {\\boldsymbol{x}_{t+1-h_d}}\\end{array}\\right].$$\n",
    "\n",
    "<img src=\"../images/rolling_prediction.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "> **Figure 2**: A graphical illustration of the rolling prediction scheme using BTMF (with VAR process) (green: observed data; white: missing data; red: prediction).\n",
    "\n",
    "In [Yu et al., 2016](https://www.cs.utexas.edu/~rofuyu/papers/tr-mf-nips.pdf), to avoid overfitting and reduce the number of parameters, the coefficient matrix in TRMF is further assumed to be a diagonal $A_k=\\text{diag}(\\boldsymbol{\\theta}_{k})$. Therefore, they have\n",
    "$$\\boldsymbol{x}_{t+1}=\\boldsymbol{\\theta}_{1}\\circledast\\boldsymbol{x}_{t+1-h_1}+\\cdots+\\boldsymbol{\\theta}_{d}\\circledast\\boldsymbol{x}_{t+1-h_d}+\\boldsymbol{\\epsilon}_t,~~~~(4)$$\n",
    "where the symbol $\\circledast$ denotes the element-wise Hadamard product. However, unlike Equation (4), a vector autoregressive (VAR) model in Equation (3) is actually more powerful for capturing multivariate time series patterns. \n",
    "\n",
    "<img src=\"../images/rolling_prediction_strategy.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "> **Figure 3**: A graphical illustration of the rolling prediction scheme using BTMF (with AR process) (green: observed data; white: missing data; red: prediction).\n",
    "\n",
    "In the following, we first introduce a Bayesian temporal matrix factorization model with an autoregressive model given in Equation (4), and then discuss another model with a vector autoregressive (VAR) model shown in Equation (3).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Bayesian Temporal Matrix Factorization with Autoregressive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Bayesian Temporal Matrix Factorization with Vector Autoregressive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Model Specification\n",
    "\n",
    "Following the general Bayesian probabilistic matrix factorization models (e.g., BPMF proposed by [Salakhutdinov & Mnih, 2008](https://www.cs.toronto.edu/~amnih/papers/bpmf.pdf)), we assume that each observed entry in $Y$ follows a Gaussian distribution with precision $\\tau$:\n",
    "$$y_{i,t}\\sim\\mathcal{N}\\left(\\boldsymbol{w}_i^\\top\\boldsymbol{x}_t,\\tau^{-1}\\right),\\quad \\left(i,t\\right)\\in\\Omega.$$\n",
    "\n",
    "On the spatial dimension, we use a simple Gaussian factor matrix without imposing any dependencies explicitly:\n",
    "$$\\boldsymbol{w}_i\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_{w},\\Lambda_w^{-1}\\right),$$\n",
    "and we place a conjugate Gaussian-Wishart prior on the mean vector and the precision matrix:\n",
    "$$\\boldsymbol{\\mu}_w | \\Lambda_w \\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_0,(\\beta_0\\Lambda_w)^{-1}\\right),\\Lambda_w\\sim\\mathcal{W}\\left(W_0,\\nu_0\\right),$$\n",
    "where $\\boldsymbol{\\mu}_0\\in \\mathbb{R}^{R}$ is a mean vector, $\\mathcal{W}\\left(W_0,\\nu_0\\right)$ is a Wishart distribution with a $R\\times R$ scale matrix $W_0$ and $\\nu_0$ degrees of freedom.\n",
    "\n",
    "In modeling the temporal factor matrix $X$, we re-write the VAR process as:\n",
    "$$\\boldsymbol{x}_{t}\\sim\\left\\{\\begin{array}{ll}\n",
    "\\mathcal{N}\\left(\\boldsymbol{0},I_R\\right),&\\text{if $t\\in\\{1,2,...,h_d\\}$}, \\\\\n",
    "\\mathcal{N}\\left(A^\\top \\boldsymbol{v}_{t},\\Sigma\\right),&\\text{otherwise},\\\\\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "Since the mean vector is defined by VAR, we need to place the conjugate matrix normal inverse Wishart (MNIW) prior on the coefficient matrix $A$ and the covariance matrix $\\Sigma$ as follows,\n",
    "$$A\\sim\\mathcal{MN}_{(Rd)\\times R}\\left(M_0,\\Psi_0,\\Sigma\\right),\\quad\n",
    "\\Sigma \\sim\\mathcal{IW}\\left(S_0,\\nu_0\\right), \\\\$$\n",
    "where the probability density function for the $Rd$-by-$R$ random matrix $A$ has the form:\n",
    "$$\\begin{array}{l}\n",
    "&p\\left(A\\mid M_0,\\Psi_0,\\Sigma\\right) \\\\\n",
    "=&\\left(2\\pi\\right)^{-R^2d/2}\\left|\\Psi_0\\right|^{-Rd/2}\\left|\\Sigma\\right|^{-R/2} \\\\\n",
    "&\\times \\exp\\left(-\\frac{1}{2}\\text{tr}\\left[\\Psi_{0}^{-1}\\left(A-M_0\\right)^{\\top}\\Sigma^{-1}\\left(A-M_0\\right)\\right]\\right), \\\\\n",
    "\\end{array}$$\n",
    "where $\\Psi_0\\in\\mathbb{R}^{(Rd)\\times (Rd)}$ and $\\Sigma\\in\\mathbb{R}^{R\\times R}$ are played as covariance matrices.\n",
    "\n",
    "For the only remaining parameter $\\tau$, we place a Gamma prior  $\\tau\\sim\\text{Gamma}\\left(\\alpha,\\beta\\right)$ where $\\alpha$ and $\\beta$ are the shape and rate parameters, respectively. \n",
    "\n",
    "The above specifies the full generative process of BTMF, and we could also see the Bayesian graphical model shown in Figure 4. Several parameters are introduced to define the prior distributions for hyperparameters, including $\\boldsymbol{\\mu}_{0}$, $W_0$, $\\nu_0$, $\\beta_0$, $\\alpha$, $\\beta$, $M_0$, $\\Psi_0$, and $S_0$. These parameters need to provided in advance when training the model. However, it should be noted that the specification of these parameters has little impact on the final results, as the training data will play a much more important role in defining the posteriors of the hyperparameters.\n",
    "\n",
    "<img src=\"../images/btmf_net.png\" alt=\"drawing\" width=\"450\"/>\n",
    "\n",
    "> **Figure 4**: An overview graphical model of BTMF (time lag set: $\\left\\{1,2,...,d\\right\\}$). The shaded nodes ($y_{i,t}$) are the observed data in $\\Omega$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Model Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the complex structure of BTMF, it is intractable to write down the posterior distribution. Here we rely on the MCMC technique for Bayesian learning. In detail, we introduce a Gibbs sampling algorithm by deriving the full conditional distributions for all parameters and hyperparameters. Thanks to the use of conjugate priors in Figure 4, we can actually write down all the conditional distributions analytically. Below we summarize the Gibbs sampling procedure.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Sampling Factor Matrix $W$ and Its Hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For programming convenience, we use $W\\in\\mathbb{R}^{N\\times R}$ to replace $W\\in\\mathbb{R}^{R\\times N}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv as inv\n",
    "from numpy.random import multivariate_normal as mvnrnd\n",
    "from scipy.stats import wishart\n",
    "\n",
    "def kr_prod(a, b):\n",
    "    return np.einsum('ir, jr -> ijr', a, b).reshape(a.shape[0] * b.shape[0], -1)\n",
    "\n",
    "def cov_mat(mat):\n",
    "    dim1, dim2 = mat.shape\n",
    "    new_mat = np.zeros((dim2, dim2))\n",
    "    mat_bar = np.mean(mat, axis = 0)\n",
    "    for i in range(dim1):\n",
    "        new_mat += np.einsum('i, j -> ij', mat[i, :] - mat_bar, mat[i, :] - mat_bar)\n",
    "    return new_mat\n",
    "\n",
    "def sample_factor_w(sparse_mat, binary_mat, W, X, tau):\n",
    "    \"\"\"Sampling N-by-R factor matrix W and its hyperparameters (mu_w, Lambda_w).\"\"\"\n",
    "    \n",
    "    dim1, rank = W.shape\n",
    "    beta0 = 1\n",
    "    nu0 = rank\n",
    "    W0 = np.eye(rank)\n",
    "    \n",
    "    W_bar = np.mean(W, axis = 0)\n",
    "    var_mu_hyper = (dim1 * W_bar)/(dim1 + beta0)\n",
    "    var_W_hyper = inv(inv(W0) + cov_mat(W) + dim1 * beta0/(dim1 + beta0) * np.outer(W_bar, W_bar))\n",
    "    var_Lambda_hyper = wishart(df = dim1 + nu0, scale = var_W_hyper, seed = None).rvs()\n",
    "    var_mu_hyper = mvnrnd(var_mu_hyper, inv((dim1 + beta0) * var_Lambda_hyper))\n",
    "    \n",
    "    var1 = X.T\n",
    "    var2 = kr_prod(var1, var1)\n",
    "    var3 = (tau * np.matmul(var2, binary_mat.T).reshape([rank, rank, dim1]) \n",
    "            + np.dstack([var_Lambda_hyper] * dim1))\n",
    "    var4 = (tau * np.matmul(var1, sparse_mat.T)\n",
    "            + np.dstack([np.matmul(var_Lambda_hyper, var_mu_hyper)] * dim1)[0, :, :])\n",
    "    for i in range(dim1):\n",
    "        inv_var_Lambda = inv(var3[:, :, i])\n",
    "        W[i, :] = mvnrnd(np.matmul(inv_var_Lambda, var4[:, i]), inv_var_Lambda)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Sampling VAR Coefficients $A$ and Its Hyperparameters\n",
    "\n",
    "**Foundations of VAR**\n",
    "\n",
    "Vector autoregression (VAR) is a multivariate extension of autoregression (AR). Formally, VAR for $R$-dimensional vectors $\\boldsymbol{x}_{t}$ can be written as follows,\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{x}_{t}&=A_{1} \\boldsymbol{x}_{t-h_1}+\\cdots+A_{d} \\boldsymbol{x}_{t-h_d}+\\boldsymbol{\\epsilon}_{t}, \\\\\n",
    "&= A^\\top \\boldsymbol{v}_{t}+\\boldsymbol{\\epsilon}_{t},~t=h_d+1, \\ldots, T, \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "A=\\left[A_{1}, \\ldots, A_{d}\\right]^{\\top} \\in \\mathbb{R}^{(R d) \\times R},\\quad \\boldsymbol{v}_{t}=\\left[\\begin{array}{c}{\\boldsymbol{x}_{t-h_1}} \\\\ {\\vdots} \\\\ {\\boldsymbol{x}_{t-h_d}}\\end{array}\\right] \\in \\mathbb{R}^{(R d) \\times 1}.\n",
    "\\end{equation}\n",
    "\n",
    "In the following, if we define\n",
    "\\begin{equation}\n",
    "Z=\\left[\\begin{array}{c}{\\boldsymbol{x}_{h_d+1}^{\\top}} \\\\ {\\vdots} \\\\ {\\boldsymbol{x}_{T}^{\\top}}\\end{array}\\right] \\in \\mathbb{R}^{(T-h_d) \\times R},\\quad Q=\\left[\\begin{array}{c}{\\boldsymbol{v}_{h_d+1}^{\\top}} \\\\ {\\vdots} \\\\ {\\boldsymbol{v}_{T}^{\\top}}\\end{array}\\right] \\in \\mathbb{R}^{(T-d) \\times(R d)},\n",
    "\\end{equation}\n",
    "then, we could write the above mentioned VAR as\n",
    "\\begin{equation}\n",
    "\\underbrace{Z}_{(T-h_d)\\times R}\\approx \\underbrace{Q}_{(T-h_d)\\times (Rd)}\\times \\underbrace{A}_{(Rd)\\times R}.\n",
    "\\end{equation}\n",
    "\n",
    "> To include temporal factors $\\boldsymbol{x}_{t},t=1,...,h_d$, we also define $$Z_0=\\left[\\begin{array}{c}{\\boldsymbol{x}_{1}^{\\top}} \\\\ {\\vdots} \\\\ {\\boldsymbol{x}_{h_d}^{\\top}}\\end{array}\\right] \\in \\mathbb{R}^{h_d \\times R}.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a Bayesian VAR on temporal factors $\\boldsymbol{x}_{t}$**\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{x}_{t}&\\sim\\begin{cases}\\mathcal{N}\\left(A^\\top \\boldsymbol{v}_{t},\\Sigma\\right),~\\text{if $t\\in\\left\\{h_d+1,...,T\\right\\}$},\\\\{\\mathcal{N}\\left(\\boldsymbol{0},I_R\\right),~\\text{otherwise}}.\\end{cases}\\\\\n",
    "A&\\sim\\mathcal{MN}_{(Rd)\\times R}\\left(M_0,\\Psi_0,\\Sigma\\right), \\\\\n",
    "\\Sigma &\\sim\\mathcal{IW}\\left(S_0,\\nu_0\\right), \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&\\mathcal{M N}_{(R d) \\times R}\\left(A | M_{0}, \\Psi_{0}, \\Sigma\\right)\\\\\n",
    "\\propto|&\\Sigma|^{-R d / 2} \\exp \\left(-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}\\left(A-M_{0}\\right)^{\\top} \\Psi_{0}^{-1}\\left(A-M_{0}\\right)\\right]\\right), \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "\\mathcal{I} \\mathcal{W}\\left(\\Sigma | S_{0}, \\nu_{0}\\right) \\propto|\\Sigma|^{-\\left(\\nu_{0}+R+1\\right) / 2} \\exp \\left(-\\frac{1}{2} \\operatorname{tr}\\left(\\Sigma^{-1}S_{0}\\right)\\right).\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Likelihood from temporal factors $\\boldsymbol{x}_{t}$**\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&\\mathcal{L}\\left(X\\mid A,\\Sigma\\right) \\\\\n",
    "\\propto &\\prod_{t=1}^{h_d}p\\left(\\boldsymbol{x}_{t}\\mid \\Sigma\\right)\\times \\prod_{t=h_d+1}^{T}p\\left(\\boldsymbol{x}_{t}\\mid A,\\Sigma\\right) \\\\\n",
    "\\propto &\\left|\\Sigma\\right|^{-T/2}\\exp\\left\\{-\\frac{1}{2}\\sum_{t=h_d+1}^{T}\\left(\\boldsymbol{x}_{t}-A^\\top \\boldsymbol{v}_{t}\\right)^\\top\\Sigma^{-1}\\left(\\boldsymbol{x}_{t}-A^\\top \\boldsymbol{v}_{t}\\right)\\right\\} \\\\\n",
    "\\propto &\\left|\\Sigma\\right|^{-T/2}\\exp\\left\\{-\\frac{1}{2}\\text{tr}\\left[\\Sigma^{-1}\\left(Z_0^\\top Z_0+\\left(Z-QA\\right)^\\top \\left(Z-QA\\right)\\right)\\right]\\right\\}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Posterior distribution**\n",
    "\n",
    "Consider\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&\\left(A-M_{0}\\right)^{\\top} \\Psi_{0}^{-1}\\left(A-M_{0}\\right)+S_0+Z_0^\\top Z_0+\\left(Z-QA\\right)^\\top \\left(Z-QA\\right) \\\\\n",
    "=&A^\\top\\left(\\Psi_0^{-1}+Q^\\top Q\\right)A-A^\\top\\left(\\Psi_0^{-1}M_0+Q^\\top Z\\right) \\\\\n",
    "&-\\left(\\Psi_0^{-1}M_0+Q^\\top Z\\right)^\\top A \\\\\n",
    "&+\\left(\\Psi_0^{-1}M_0+Q^\\top Z\\right)^\\top\\left(\\Psi_0^{-1}+Q^\\top Q\\right)\\left(\\Psi_0^{-1}M_0+Q^\\top Z\\right) \\\\\n",
    "&-\\left(\\Psi_0^{-1}M_0+Q^\\top Z\\right)^\\top\\left(\\Psi_0^{-1}+Q^\\top Q\\right)\\left(\\Psi_0^{-1}M_0+Q^\\top Z\\right) \\\\\n",
    "&+M_0^\\top\\Psi_0^{-1}M_0+S_0+Z_0^\\top Z_0+Z^\\top Z \\\\\n",
    "=&\\left(A-M^{*}\\right)^\\top\\left(\\Psi^{*}\\right)^{-1}\\left(A-M^{*}\\right)+S^{*}, \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "which is in the form of $\\mathcal{MN}\\left(\\cdot\\right)$ and $\\mathcal{IW}\\left(\\cdot\\right)$.\n",
    "\n",
    "The $Rd$-by-$R$ matrix $A$ has a matrix normal distribution, and $R$-by-$R$ covariance matrix $\\Sigma$ has an inverse Wishart distribution, that is,\n",
    "\\begin{equation}\n",
    "A \\sim \\mathcal{M N}_{(R d) \\times R}\\left(M^{*}, \\Psi^{*}, \\Sigma\\right), \\quad \\Sigma \\sim \\mathcal{I} \\mathcal{W}\\left(S^{*}, \\nu^{*}\\right),\n",
    "\\end{equation}\n",
    "with\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "{\\Psi^{*}=\\left(\\Psi_{0}^{-1}+Q^{\\top} Q\\right)^{-1}}, \\\\ {M^{*}=\\Psi^{*}\\left(\\Psi_{0}^{-1} M_{0}+Q^{\\top} Z\\right)}, \\\\ {S^{*}=S_{0}+Z^\\top Z+M_0^\\top\\Psi_0^{-1}M_0-\\left(M^{*}\\right)^\\top\\left(\\Psi^{*}\\right)^{-1}M^{*}}, \\\\ \n",
    "{\\nu^{*}=\\nu_{0}+T-h_d}.\n",
    "\\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import invwishart\n",
    "\n",
    "def mat2ten(mat, tensor_size, mode):\n",
    "    index = list()\n",
    "    index.append(mode)\n",
    "    for i in range(tensor_size.shape[0]):\n",
    "        if i != mode:\n",
    "            index.append(i)\n",
    "    return np.moveaxis(np.reshape(mat, list(tensor_size[index]), order = 'F'), 0, mode)\n",
    "\n",
    "def mnrnd(M, U, V):\n",
    "    \"\"\"\n",
    "    Generate matrix normal distributed random matrix.\n",
    "    M is a m-by-n matrix, U is a m-by-m matrix, and V is a n-by-n matrix.\n",
    "    \"\"\"\n",
    "    dim1, dim2 = M.shape\n",
    "    X0 = np.random.rand(dim1, dim2)\n",
    "    P = np.linalg.cholesky(U)\n",
    "    Q = np.linalg.cholesky(V)\n",
    "    return M + np.matmul(np.matmul(P, X0), Q.T)\n",
    "\n",
    "def sample_var_coefficient(X, time_lags):\n",
    "    \n",
    "    dim2, rank = X.shape\n",
    "    d = time_lags.shape[0]\n",
    "    nu0 = rank\n",
    "    S0 = np.eye(rank)\n",
    "    Psi0 = np.eye(rank * d)\n",
    "    M0 = np.zeros((rank * d, rank))\n",
    "    \n",
    "    Z_mat = X[np.max(time_lags) : dim2, :]\n",
    "    Q_mat = np.zeros((dim2 - np.max(time_lags), rank * d))\n",
    "    for t in range(np.max(time_lags), dim2):\n",
    "        Q_mat[t - np.max(time_lags), :] = X[t - time_lags, :].reshape([rank * d])\n",
    "    var_Psi = inv(inv(Psi0) + np.matmul(Q_mat.T, Q_mat))\n",
    "    var_M = np.matmul(var_Psi, np.matmul(inv(Psi0), M0) + np.matmul(Q_mat.T, Z_mat))\n",
    "    var_S = (S0 + np.matmul(Z_mat.T, Z_mat) + np.matmul(np.matmul(M0.T, inv(Psi0)), M0) \n",
    "             - np.matmul(np.matmul(var_M.T, inv(var_Psi)), var_M))\n",
    "    Sigma = invwishart(df = nu0 + dim2 - np.max(time_lags), scale = var_S, seed = None).rvs()\n",
    "    return mat2ten(mnrnd(var_M, var_Psi, Sigma).T, np.array([rank, rank, d]), 0), Sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Sampling Factor Matrix $X$\n",
    "\n",
    "**Posterior distribution**\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "y_{it}&\\sim\\mathcal{N}\\left(\\boldsymbol{w}_{i}^\\top\\boldsymbol{x}_{t},\\tau^{-1}\\right),~\\left(i,t\\right)\\in\\Omega, \\\\\n",
    "\\boldsymbol{x}_{t}&\\sim\\begin{cases}\\mathcal{N}\\left(\\sum_{k=1}^{d}A_{k} \\boldsymbol{x}_{t-h_k},\\Sigma\\right),~\\text{if $t\\in\\left\\{h_d+1,...,T\\right\\}$},\\\\{\\mathcal{N}\\left(\\boldsymbol{0},I\\right),~\\text{otherwise}}.\\end{cases}\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "If $t\\in\\left\\{1,...,h_d\\right\\}$, parameters of the posterior distribution $\\mathcal{N}\\left(\\boldsymbol{x}_{t}\\mid \\boldsymbol{\\mu}_{t}^{*},\\Sigma_{t}^{*}\\right)$ are\n",
    "\\footnotesize{\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\Sigma_{t}^{*}&=\\left(\\sum_{k=1, h_{d}<t+h_{k} \\leq T}^{d} {A}_{k}^{\\top} \\Sigma^{-1} A_{k}+\\tau\\sum_{i:(i,t)\\in\\Omega}\\boldsymbol{w}_{i}\\boldsymbol{w}_{i}^\\top+I\\right)^{-1}, \\\\\n",
    "\\boldsymbol{\\mu}_{t}^{*}&=\\Sigma_{t}^{*}\\left(\\sum_{k=1, h_{d}<t+h_{k} \\leq T}^{d} A_{k}^{\\top} \\Sigma^{-1} \\boldsymbol{\\psi}_{t+h_{k}}+\\tau\\sum_{i:(i,t)\\in\\Omega}\\boldsymbol{w}_{i}y_{it}\\right). \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "If $t\\in\\left\\{h_d+1,...,T\\right\\}$, then parameters of the posterior distribution $\\mathcal{N}\\left(\\boldsymbol{x}_{t}\\mid \\boldsymbol{\\mu}_{t}^{*},\\Sigma_{t}^{*}\\right)$ are\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\Sigma_{t}^{*}&=\\left(\\sum_{k=1, h_{d}<t+h_{k} \\leq T}^{d} {A}_{k}^{\\top} \\Sigma^{-1} A_{k}+\\tau\\sum_{i:(i,t)\\in\\Omega}\\boldsymbol{w}_{i}\\boldsymbol{w}_{i}^\\top+\\Sigma^{-1}\\right)^{-1}, \\\\\n",
    "\\boldsymbol{\\mu}_{t}^{*}&=\\Sigma_{t}^{*}\\left(\\sum_{k=1, h_{d}<t+h_{k} \\leq T}^{d} A_{k}^{\\top} \\Sigma^{-1} \\boldsymbol{\\psi}_{t+h_{k}}+\\tau\\sum_{i:(i,t)\\in\\Omega}\\boldsymbol{w}_{i}y_{it}+\\Sigma^{-1}\\sum_{k=1}^{d}A_{k}\\boldsymbol{x}_{t-h_k}\\right), \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where\n",
    "$$\\boldsymbol{\\psi}_{t+h_k}=\\boldsymbol{x}_{t+h_k}-\\sum_{l=1,l\\neq k}^{d}A_{l}\\boldsymbol{x}_{t+h_k-h_l}.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ten2mat(tensor, mode):\n",
    "    return np.reshape(np.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1), order = 'F')\n",
    "\n",
    "def sample_factor_x(sparse_mat, binary_mat, time_lags, W, X, tau, A, Lambda_x):\n",
    "    \n",
    "    dim2, rank = X.shape\n",
    "    d = time_lags.shape[0]\n",
    "    \n",
    "    var1 = W.T\n",
    "    var2 = kr_prod(var1, var1)\n",
    "    var3 = tau * np.matmul(var2, binary_mat).reshape([rank, rank, dim2]) + np.dstack([Lambda_x] * dim2)\n",
    "    var4 = tau * np.matmul(var1, sparse_mat)\n",
    "    for t in range(dim2):\n",
    "        Mt = np.zeros((rank, rank))\n",
    "        Nt = np.zeros(rank)\n",
    "        if t < np.max(time_lags):\n",
    "            Qt = np.zeros(rank)\n",
    "        else:\n",
    "            Qt = np.matmul(Lambda_x, np.matmul(ten2mat(A, 0), X[t - time_lags, :].reshape([rank * d])))\n",
    "        if t < dim2 - np.min(time_lags):\n",
    "            if t >= np.max(time_lags) and t < dim2 - np.max(time_lags):\n",
    "                index = list(range(0, d))\n",
    "            else:\n",
    "                index = list(np.where((t + time_lags >= np.max(time_lags)) & (t + time_lags < dim2)))[0]\n",
    "            for k in index:\n",
    "                Ak = A[:, :, k]\n",
    "                Mt += np.matmul(np.matmul(Ak.T, Lambda_x), Ak)\n",
    "                A0 = A.copy()\n",
    "                A0[:, :, k] = 0\n",
    "                var5 = (X[t + time_lags[k], :] \n",
    "                        - np.matmul(ten2mat(A0, 0), X[t + time_lags[k] - time_lags, :].reshape([rank * d])))\n",
    "                Nt += np.matmul(np.matmul(Ak.T, Lambda_x), var5)\n",
    "        var_mu = var4[:, t] + Nt + Qt\n",
    "        if t < np.max(time_lags):\n",
    "            inv_var_Lambda = inv(var3[:, :, t] + Mt - Lambda_x + np.eye(rank))\n",
    "        else:\n",
    "            inv_var_Lambda = inv(var3[:, :, t] + Mt)\n",
    "        X[t, :] = mvnrnd(np.matmul(inv_var_Lambda, var_mu), inv_var_Lambda)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Sampling Precision $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_precision_tau(sparse_mat, mat_hat, position):\n",
    "    alpha = 1e-6\n",
    "    beta = 1e-6\n",
    "    return np.random.gamma(alpha + 0.5 * sparse_mat[position].shape[0], \n",
    "                           1/(beta + 0.5 * np.sum((sparse_mat - mat_hat)[position] ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) BTMF Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BTMF(dense_mat, sparse_mat, init, rank, time_lags, maxiter1, maxiter2):\n",
    "    \"\"\"Bayesian Temporal Matrix Factorization, BTMF.\"\"\"\n",
    "    \n",
    "    W = init[\"W\"]\n",
    "    X = init[\"X\"]\n",
    "    dim1, dim2 = sparse_mat.shape\n",
    "    d = time_lags.shape[0]\n",
    "    pos = np.where((dense_mat != 0) & (sparse_mat == 0))\n",
    "    position = np.where(sparse_mat != 0)\n",
    "    binary_mat = np.zeros((dim1, dim2))\n",
    "    binary_mat[position] = 1\n",
    "    \n",
    "    tau = 1\n",
    "    W_plus = np.zeros((dim1, rank))\n",
    "    X_plus = np.zeros((dim2, rank))\n",
    "    A_plus = np.zeros((rank, rank, d))\n",
    "    mat_hat_plus = np.zeros((dim1, dim2))\n",
    "    for it in range(maxiter1):\n",
    "        W = sample_factor_w(sparse_mat, binary_mat, W, X, tau)\n",
    "        A, Sigma = sample_var_coefficient(X, time_lags)\n",
    "        X = sample_factor_x(sparse_mat, binary_mat, time_lags, W, X, tau, A, inv(Sigma))\n",
    "        mat_hat = np.matmul(W, X.T)\n",
    "        if it + 1 > maxiter1 - maxiter2:\n",
    "            W_plus += W\n",
    "            X_plus += X\n",
    "            A_plus += A\n",
    "            mat_hat_plus += mat_hat\n",
    "        \n",
    "        tau = sample_precision_tau(sparse_mat, mat_hat, position)\n",
    "        rmse = np.sqrt(np.sum((dense_mat[pos] - mat_hat[pos]) ** 2)/dense_mat[pos].shape[0])\n",
    "        if (it + 1) % 1 == 0 and it < maxiter1 - maxiter2:\n",
    "            print('Iteration: {}'.format(it + 1))\n",
    "            print('RMSE: {:.6}'.format(rmse))\n",
    "            print()\n",
    "\n",
    "    mat_hat = mat_hat_plus/maxiter2\n",
    "    final_mape = np.sum(np.abs(dense_mat[pos] - mat_hat[pos])/dense_mat[pos])/dense_mat[pos].shape[0]\n",
    "    final_rmse = np.sqrt(np.sum((dense_mat[pos] - mat_hat[pos]) ** 2)/dense_mat[pos].shape[0])\n",
    "    print('Imputation MAPE: {:.6}'.format(final_mape))\n",
    "    print('Imputation RMSE: {:.6}'.format(final_rmse))\n",
    "    print()\n",
    "    \n",
    "    return mat_hat, W_plus/maxiter2, X_plus/maxiter2, A_plus/maxiter2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Toy Example: Spatiotemporal Data Imputation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/tensor.mat')\n",
    "tensor = tensor['tensor']\n",
    "random_matrix = scipy.io.loadmat('../datasets/Guangzhou-data-set/random_matrix.mat')\n",
    "random_matrix = random_matrix['random_matrix']\n",
    "random_tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/random_tensor.mat')\n",
    "random_tensor = random_tensor['random_tensor']\n",
    "\n",
    "dense_mat = tensor.reshape([tensor.shape[0], tensor.shape[1] * tensor.shape[2]])\n",
    "missing_rate = 0.4\n",
    "\n",
    "# =============================================================================\n",
    "### Random missing (RM) scenario\n",
    "### Set the RM scenario by:\n",
    "binary_mat = (np.round(random_tensor + 0.5 - missing_rate)\n",
    "              .reshape([random_tensor.shape[0], random_tensor.shape[1] * random_tensor.shape[2]]))\n",
    "# =============================================================================\n",
    "\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "RMSE: 5.83457\n",
      "\n",
      "Iteration: 2\n",
      "RMSE: 5.76757\n",
      "\n",
      "Iteration: 3\n",
      "RMSE: 5.59097\n",
      "\n",
      "Iteration: 4\n",
      "RMSE: 5.06125\n",
      "\n",
      "Iteration: 5\n",
      "RMSE: 4.77557\n",
      "\n",
      "Iteration: 6\n",
      "RMSE: 4.61404\n",
      "\n",
      "Iteration: 7\n",
      "RMSE: 4.58146\n",
      "\n",
      "Iteration: 8\n",
      "RMSE: 4.53475\n",
      "\n",
      "Iteration: 9\n",
      "RMSE: 4.48575\n",
      "\n",
      "Iteration: 10\n",
      "RMSE: 4.47013\n",
      "\n",
      "Iteration: 11\n",
      "RMSE: 4.4566\n",
      "\n",
      "Iteration: 12\n",
      "RMSE: 4.41712\n",
      "\n",
      "Iteration: 13\n",
      "RMSE: 4.38047\n",
      "\n",
      "Iteration: 14\n",
      "RMSE: 4.35006\n",
      "\n",
      "Iteration: 15\n",
      "RMSE: 4.32793\n",
      "\n",
      "Iteration: 16\n",
      "RMSE: 4.29772\n",
      "\n",
      "Iteration: 17\n",
      "RMSE: 4.27643\n",
      "\n",
      "Iteration: 18\n",
      "RMSE: 4.26555\n",
      "\n",
      "Iteration: 19\n",
      "RMSE: 4.25265\n",
      "\n",
      "Iteration: 20\n",
      "RMSE: 4.244\n",
      "\n",
      "Iteration: 21\n",
      "RMSE: 4.23248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 80\n",
    "time_lags = np.array([1, 2, 144])\n",
    "init = {\"W\": 0.1 * np.random.rand(dim1, rank), \"X\": 0.1 * np.random.rand(dim2, rank)}\n",
    "maxiter1 = 1100\n",
    "maxiter2 = 100\n",
    "BTMF(dense_mat, sparse_mat, init, rank, time_lags, maxiter1, maxiter2)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
